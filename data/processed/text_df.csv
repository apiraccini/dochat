id,article
0,"# Explaining prediction models and individual predictions with feature contributions Erik Strumbelj E. Strumbelj (). I. Kononenko Faculty of Computer and Information Science, University of Ljubljana, Trzaska 25, 1000 Ljubljana, Slovenia 1 Igor Kononenko E. Strumbelj (). I. Kononenko Faculty of Computer and Information Science, University of Ljubljana, Trzaska 25, 1000 Ljubljana, Slovenia 1 Footnote 1: email: erik.strumbelj@fi.uni-lj.si ###### Abstract We present a sensitivity analysis-based method for explaining prediction models that can be applied to any type of classification or regression model. Its advantage over existing general methods is that all subsets of input features are perturbed, so interactions and redundancies between features are taken into account. Furthermore, when explaining an additive model, the method is equivalent to commonly used additive model-specific methods. We illustrate the method's usefulness with examples from artificial and real-world data sets and an empirical analysis of running times. Results from a controlled experiment with 122 participants suggest that the method's explanations improved the participants' understanding of the model. Knowledge discovery Data mining Visualization Interpretability Decision support 12 November 2012 / Revised: 2 August 2013 / Accepted: 17 August 2013 ## 1 Introduction Prediction models are an important component of decision support systems. Applications range from credit scoring [11] and fraud detection [5] to financial auditing [4] and efficiency analysis [18]. In such applications, model interpretability is often as important if not more important than prediction accuracy. Some more difficult to interpret models require additional post-processing to (a) obtain a better understanding of the model and (b) increase the end-user's level of trust in the model. The latter is especially important in risk-sensitive domains such as finance and medicine, where experts are reluctant to trust prediction model's predictions without an additional explanation. Most explanation methods are model-specific. Some models, such as decision and regression trees, rules [6; 8], and nearest neighbors-based methods, are self-explanatory. Complex models, such as artificial neural networks and SVM (support vector machines), received the most attention, because they are often very successful but difficult to interpret (see [28], and references therein). Linear regression and other additive models can be additionally explained by plotting the marginal effect of each input variable. For additive models, a prediction is the sum of individual marginal effects, which makes such visualizations a tool for graphical computation of predictions--a nomogram. This fact has been exploited to provide an explanation for the naive Bayes classifier [3; 17; 23], linear SVM [14], logistic regression [21], Cox regression models [15], and additive models in general [26]. This paper focuses on explanation methods that can be applied to prediction models of any type. Such general approaches must treat the model as a black box and are thus restricted to changing the inputs and observing the changes in the output. While not being able to exploit the specifics of the model, such methods have the advantage of being applicable to any type of model. This facilitates comparison of different types of models and, in practical applications, eliminates the need to replace the explanation method when the underlying model is replaced. The key component of general explanations is the contributions of individual input features. A prediction is explained by assigning to each feature a number which denote its influence. For each feature, such contributions can be aggregated to plot the feature's average contribution against the feature's value. This provides an overview of the model and is similar to plotting the marginal effect for an additive model. We begin with a simple illustrative example--a linear regression model: \[f(x)=f(x_{1},\ldots,x_{n})\approx y=\beta_{0}+\beta_{1}x_{1}+\cdots+\beta_{n} x_{n}.\] If the input features are standardized, the coefficient \(\beta_{i}\) can be interpreted as the \(i\)th feature's global importance. However, in practice, we are more interested in how a particular value influences the prediction. We turn to the following expression \[\varphi_{i}(x)=\beta_{i}x_{i}-\beta_{i}E[X_{i}],\] (1) which is also known as the situational importance of \(X_{i}=x_{i}\)[1]. The Computing the contributions for our illustrative example was simple, because the model is known and the features do not interact. Therefore, the contribution of some \(x_{i}\) is the same across all instances, regardless of the values of other features. In our problem setting, however, the model is unknown and no assumptions are made other than that the model maps from some known input feature space to a known codomain. These restrictions are necessary for the method to be general, but restrict us to changing the inputs and observing the outputs. Previous general approaches [19; 24] tackled the problem in the following way: \[\varphi_{i}(x) = f(x_{1},\ldots,x_{n})-E[f(x_{1},\ldots,X_{i},\ldots,x_{n})]\] (2) Equation (2) is the difference between a prediction for an instance and the expected prediction for the same instance if the \(i\)th feature had not been known. In practice, expression Eq. (2) is not difficult to approximate (or compute, if the feature's domain is finite)--we have to perturb the values of the \(i\)th feature, while the values of other input features remain fixed, and then average the prediction. Additionally, if \(f\) is an additive model, Eq. (2) is equivalent to Eq. (1) used for additive models. In the experimental part of the paper, we provide an empirical analysis of running times, illustrative examples, and the results of a controlled experiment of the usefulness of the contribution-based instance explanations. The remainder of the paper is organized as follows. In Sect. 2, we provide the essential background from [28] and [29]. In Sect. 3, we describe and improve the approximation algorithm for computing a feature's contribution for an instance [28] and the average contribution of a feature's value. Section 4 is dedicated to experimental results and visual inspection of instance and model visualizations. Section 5 concludes the paper. ## 2 Computing a feature's contribution The following notation will be used throughout the paper. Let \(\mathcal{X}=[0,1]^{n}\) be our feature space, \(Y\) the target variable, and \(\{y_{i};x_{1,i},x_{2,i},\ldots,x_{n,i}\}_{i=1}^{M}\) a data set of \(M\) instances. The function \(f:\mathcal{X}\rightarrow\Re\) represents the model that is used to predict the value of the target variable for an instance \(x\in\mathcal{X}\). In the classification case, we take the one-vs-all approach--we choose and observe the prediction for a single class value. Any class value of interest can be chosen or explanations can be generated for several values. We will refer to the chosen value as the point-of-view class. which uniquely determines the interactions: \[\mathcal{I}_{Q}(x)=\Delta_{Q}(x)-\sum_{W\subset Q}\mathcal{I}_{W}(x).\] (7) Finally, each interaction is divided among the participating feature values, which defines the contribution: \[\varphi_{i}(x)=\sum_{W\subseteq S\backslash\{i\}}\frac{\mathcal{I}_{W\cup\{i\} }(x)}{|W|+1}.\] (8) This leads to the following explicit definition (see [28] for proof): \[\varphi_{i}(x)=\sum_{Q\subseteq S\backslash\{i\}}\frac{|Q|!(|S|-|Q|-1)!}{|S|!} (\Delta_{Q\cup\{i\}}(x)-\Delta_{Q}(x)).\] (9) Equation (9) is equivalent to the Shapley value [25], a concept from coalitional game theory. In a coalitional game, it is usually assumed that \(n\) players form a grand coalition that has a certain worth (in our case, \(\Delta_{S}\)). We also know how much each smaller (subset) coalition would have been worth (\(\Delta_{Q}\), \(Q\subset S\)). The goal is to distribute the worth of the grand coalition among players in a fair way (that is, each player should receive his fair share, taking into account all sub-coalitions). The Shapley value is one such solution, and it is the unique solution that satisfies the following properties [25]: * \(\sum_{i=1}^{n}\varphi_{i}(x)=\Delta_{S}(x)\) * \(\forall W\subseteq S\backslash\{i\}:\Delta_{W}=\Delta_{W\cup\{i\}}\Rightarrow \varphi_{i}(x)=0\) * \(\forall W\subseteq S\backslash\{i,j\}:\Delta_{W\cup\{i\}}=\Delta_{W\cup\{j\}} \Rightarrow\varphi_{i}(x)=\varphi_{j}(x)\) * \(\forall x,\,y\in\mathcal{X}\): \(\varphi(x+y)=\varphi(x)+\varphi(y)\), where \(\Delta_{Q}(x+y)=\Delta_{Q}(x)+\Delta_{Q}(y)\) for all \(Q\subseteq S\) In the context of our explanation with local contributions, the properties have the following interpretation. The contributions are implicitly normalized, which makes them easier to interpret and compare. If a feature's value does not have any impact on the prediction, then it will be assigned a 0 contribution. If two features' values have the a symmetrical impact across all subsets, they will be assigned equal contributions, and the local contributions are additive across instances. ## 3 Approximation algorithm Computing Eq. (9) has an exponential time complexity, which makes the method infeasible for practical use. The following approximation is used to reduce the computational complexity. We start by writing a different but equivalent formulation of Eq. (9): \[\varphi_{i}(x)=\frac{1}{n!}\sum_{\mathcal{O}\in\pi(N)}\left(\Delta_{Pre^{i}( \mathcal{O})\cup\{i\}}-\Delta_{Pre^{i}(\mathcal{O})}\right),\ \ \ \ \ i=1,\ldots,n,\] (10) where \(\pi(n)\) is the set of all ordered permutations of the feature indices \(\{1,2,\,\ldots,n\}\) and \(Pre^{i}(\mathcal{O})\) is the set of all indices that precede \(i\) in permutation \(\mathcal{O}\in\pi(n)\). If the cost of computing the \(\Delta\)-terms would be zero, Eq. (10) could be approximated using a simple sampling algorithm, where \(\left(\Delta_{Pre^{i}(\mathcal{O})\cup\{i\}}-\Delta_{Pre^{i}(\mathcal{O})}\right)\) would be one sample (see, for example, [7]). However, the computational complexity of computing the \(\Delta\)-terms is exponential. As shown in [29], it is sufficient to limit ourselves to such distributions of instances \(p\) that individual features are distributed independently. Now, Eq. (5) can be simplified into the following: \[\Delta_{Q}(x) = f_{Q}(x)-f_{\{\}}(x)\] (11) \[= \sum_{w\in\mathcal{X};\forall i:(w_{i}=x_{i}\lor i\notin S)}p(w)f(w )-\sum_{w\in\mathcal{X};\forall i:(w_{i}=x_{i})}p(w)f(w)\] \[= \sum_{w\in\mathcal{X}}p(w)(f(w_{[w_{i}=x_{i},i\in S]})-f(w)),\] where the notation \(w_ \(Z_{i}=|\hat{\varphi}_{i}-\varphi_{i}|\) is half-normal, with \(E[Z_{i}]=\sqrt{\frac{\sigma_{i}^{2}}{m_{i}}}\sqrt{\frac{2}{\pi}}\). The expectation for the sum of absolute errors is \[E\left[\sum_{i=1}^{n}Z_{i}\right]=\sum_{i=1}n\sqrt{\frac{\sigma_{i}^{2}}{m_{i}} }\sqrt{\frac{2}{\pi}}=\sqrt{\frac{2}{\pi}}\sum_{i=1}^{n}\frac{\sigma_{i}}{ \sqrt{m_{i}}}.\] Similarly, for the sum of squared errors, we take \(Z_{i}\approx(\hat{\varphi}_{i}-\varphi_{i})^{2}\). The expectation \(E[Z_{i}^{2}]=Cov[Z_{i},Z_{i}]+2E[Z_{i}]=\frac{\sigma_{i}^{2}}{m_{i}}\). The expectation for the sum of absolute errors is \[E\left[\sum_{i=1}^{n}Z_{i}\right]=\sum_{i=1}^{n}\frac{\sigma_{i}^{2}}{m_{i}}.\] In practice, we first take samples for each input feature to obtain an initial estimate of \(\sigma_{i}\). After the minimum amounts of samples have been taken, the goal is to distribute \(m_{max}\), the total number of samples we can compute, among individual features a way that we minimize the expected error. Regardless of which error we use, a greedy approach is optimal. That is, if the current amount of samples taken for each feature are \(m_{1}\), ..., \(m_{n}\), then we should take the sample for the feature that maximizes \(\sqrt{\frac{\sigma_{i}^{2}}{m_{i}}}-\sqrt{\frac{\sigma_{i}^{2}}{m_{i}+1}}\) (or \(\frac{\sigma_{i}^{2}}{m_{i}}-\frac{\sigma_{i}^{2}}{m_{i}+1}\)). This is a direct consequence of the fact that functions \(g(z)=\frac{\sigma_{i}}{\sqrt{z}} maps its values to situational contributions of those values. Recall that, for additive models, where the features do not interact, the situational contribution \(\psi_{i}(q)\) of the \(i\)th feature's value \(q\in\mathcal{X}_{i}\) is the same for all instances. However, if the model is not additive and the features interact, then the situational contribution of a feature's value depends on the values of other features. To produce a similar plot, we average the \(i\)th feature's value \(q\)'s local contributions across all instances with that value: \[\psi_{i}(q) =\sum_{x\in\mathcal{X}\wedge x_{i}=q}p(x)\varphi_{i}(x)=\sum_{x\in \mathcal{X}}p(x)\varphi_{ For the transition from line 3 to line 4 in Eq. (14), observe the probability of the composite instance \(w_{[w_{j}=x_{j},j\in Pre^{i}(\mathcal{O})]}\) being a particular instance \(z\) if \(w\) and \(x\) are samples from \(\mathcal{X}\). For a fixed permutation \(\mathcal{O}\) and if independence of features is assumed, the probability of composing particular instance \(z\) by composing two independent samples from \(\mathcal{X}\) is \(p(z)\), the probability of drawing at random instance \(z\) from \(\mathcal{X}\). Therefore, the term \(\left(f(z_{[z_{i}=q]})-f(z)\right)\) appears in the double summation with weight \(p(z)\). To compute Eq. (14), we use a similar approximation as before. We also compute the standard deviation of samples \(\left(f(x_{[x_{i}=q]})-f(x)\right)\), which can be interpreted as the input features overall importance for the model and is shown in the model visualizations in the Experimental Results in the form of a light gray line (see Fig. 5). Let \(\psi_{i},i=1\ldots n\) be the average contribution functions. If \(f\) is additive, then it holds for each input feature \(i\) and its value \(x\) that \(\psi_{i}(q)=f_{i}(q)-E[f_{i}]\), where \(f_{i}\) are the marginal effects of individual features: \[\psi_{i}(q) =\sum_{x\in\mathcal{X}}p(x)(f(x_{[x_{i}=q]})-f(x))\] \[=\sum_{\tilde{z}\in\mathcal{X}}p( waikato.ac.nz/ml/weka/). Most can also be found at the UCI Machine Learning Repository [9]. We included ten different variations of learning algorithms for classification and seven different variations for regression (see Table 2). All-used learning algorithms were from the Weka [10] machine learning software. Unless otherwise noted, default settings were used. All experiments were run on an off-the-shelf computer with a 2.4 GHz CPU and 2 GB of RAM. ### Running times analysis ### Sampling algorithm enhancements We used the following procedure to measure the benefits of using adaptive sampling and quasi-random sampling. All regression data set/regression model and classification data set/classifier pairs were included in the experiment. For every such pair, we trained the model on 500 bootstrap samples and computed the mean-squared approximation error across all instances. We computed the error at different amounts of samples per feature and for all four combinations of the basic approximation algorithm and enhancements (both, just quasi-random, just adaptive, neither). \begin{table} \begin{tabular}{l l l l} \hline Name & \#I & \#F & Description \\ \hline _Classification_ & & & \\ cChess & 2000 & 4 & Color of 4 \(\times\) 4 chessboard point \\ cCondInd & 2000 & 8 & Conditionally independent features \\ cCross & 2000 & 6 & Even or odd quadrant in coordinate system \\ cDisjunctB & 2000 & 5 & Disjunction with binary input features \\ cDisjunctN & 2000 & 5 & Disjunction with numeric features \\ cGroup & 2000 & 4 & Clusters \\ cRandom & 2000 & 4 & Random input features \\ cRedundant & 2000 & 5 & Disjunction with redundant features \\ cSphere & 2000 & 5 & Point lies in the interior of a sphere \\ cXor & 2000 & 6 & Xor \\ _Regression_ & & & \\ rDisjunctB & 2000 & 5 & Disjunction with binary input features \\ rDisjunctN & 2000 & 5 & Disjunction with numeric features \\ rLinear & 2000 & 5 & Linear problem \\ rLinNoisy & 2000 & 5 & Linear problem with noise \\ rLocLinear & 2000 & 5 & Locally linear problem \\ rNonLinPoly & 2000 & 5 & Third degree polynomial \\ rNonLinTrig & 2000 & 5 & Trigonometric function \\ rRandom & 2000 & 4 & Random input features \\ rRedundant & 2000 & 5 & Disjunction with redundant features \\ rXor & 2000 & 6 & Xor \\ \hline \end{tabular} These data sets were constructed for the purpose of experimental verification of how general explanation methods perform on data with concepts such as disjunction, xor (exclusive or), conditionally independent features, redundant, and random features \end{table} Table 1: Number of instances (#I), total number of input features (#F), and brief description of artificial data sets The results shown in Fig. 2a suggest that both enhancements improve the efficiency of the algorithm. That is, fewer samples are needed to achieve the same approximation error. The improvement achieved with quasi-random sampling is small compared to the improvement achieved by adaptive sampling. Best results are achieved when both enhancement are used. ### Scalability We illustrate how the method scales with an increasing number of features on two additional data sets. The _linear50_ data set is a regression problem with 1,000 instances and 50 standardized numerical input features. The class value is a linear combination of features. The _datgen40_ data set is a classification problem with 1,000 instances, 40 features, and 10 classes. Note that, this data set was created using Melli's generator of rule-based data sets [22]. Both data sets are available as supplementary material. For each data set, we incrementally added features and measured the time required to compute all contributions for a single instance. Note that, the number of samples taken \(n_{max}\) was such that the probability of having a relative approximation error of more than 1 % was 5 % (relative to the absolute value of the contribution). Adaptive sampling was used, but not quasi-random sampling. The results in Fig. 2b show that contributions can be computed for these data sets in real-time for a few dozen features, regardless of the choice of the model. The differences between models are in part a consequence of different variances but mostly due to the differences in the time complexity of computing a single prediction, which is the key component of the approximation algorithm's time complexity. ### Illustrative examples We use two types of visualizations: instance visualizations and model visualizations. The former are, as the name suggests, a visualization of the features' local contributions \(\varphi\) for a particular instance. Figure 3 shows a pair of instance visualizations for the same instance from the Monks1 data set but for two different types of models. At the top of an instance visualization are the \begin{table} \begin{tabular}{l l} \hline Name & Description \\ \hline AdaBoostM1 & Boosting with Naive Bayes or decision tree as base learner \\ Bagging & Bagging with either decision tree or regression tree as base learner \\ IBk & \(k\)-Nearest Neighbors with either \(k=1\) or \(k=11\)) \\ J48 & Decision tree \\ LinearRegression & Linear regression \\ Logistic & Logistic regression \\ M5P & Regression tree \\ MultilayerPerceptron & Multi-layer artificial neural network with one hidden level \\ NaiveBayes & Naive Bayes classifier \\ SVO & Support Vector Machine with second degree polynomial kernel \\ SVMreg & Regression SVM \\ \hline \end{tabular} \end{table} Table 2: A list of learning algorithms that were included in the experiments name of the data set, the model, the point-of-view class (classification only), the model's prediction for this instance, and the actual (correct) class value. The features' names and values for the instance are listed on the left- and right-hand side, respectively. The boxes contain the features' contributions for this instance. These contributions are also plotted as bars to simplify comparison and identification of features with the largest contribution. Note that, all contributions used in the visualizations were approximated with high precision (\(P(\text{error}<10^{-4})=1-10^{-3}\)). An instance visualization reveals how individual features contribute to the model's prediction for that instance. For example, for the Monks1 data set, the class value is 1 if (attr1 = attr2 \(\vee\) attr5 = 1) and 0 otherwise. The pair of instance visualizations for the same instance from Monks1, but two different models trained on this data set provide us with additional information about how the features influence the models' prediction (see Fig. 3). Although the models are different, the general method facilitates comparison and reveals an important difference between the two models, despite their similar predictions for the same instance. Note that, one-feature-at-a-time approaches would assign a 0 contribution to all features in the artificial neural network case. Perturbing just one feature does not change the model's prediction. The second pair of instance visualizations is for two different models trained on the cRedundant data set (see Fig. 4). This data set has 5 numerical input features. The class value equals 1 if \(A_{1}>0.5\) or \(A_{2}>0.7\) or \(A_{3}<0.4\). Otherwise, it is 0. Note that, the remaining two features \(A_{4}\) and \(A_{5}\) are copies of \(A_{1}\) and \(A_{2}\), respectively, which introduces some redundancy. For this instance, the values of the first three input features are 0.96, 0.72, and 0.67. The first two features satisfy the condition, while the third does not. Given that both models are successful predictors for this data set, they have learned these concepts, and appropriately, the first two features are assigned a positive contribution, while the third contributes against the class value being 1. Note that, the artificial neural network takes into Figure 2: The left-hand side plot shows the relative approximation error against the number of samples per feature for four variants of the approximation algorithm. Errors are relative to the error at (5,000\(\times\) number of input features) samples with both enhancements. The plot on the right hand side shows the running times for computing the contributions for an instance for the datgen40 data set. The most time-consuming models for the linear50 are also included. The remaining model/dataset pairs take less time than the Naive Bayes model and were omitted account redundant features as well, while bagging with decision trees only takes into account one of each of the input features redundant copies. Although both models are equally good predictors, the explanations are different, because the explanations reveal what the models have learned. #### Model visualizations The second type visualization is the model visualization (see, for example, Fig. 4(a)). It is composed of \(n\) marginal effect plots, one for each feature (see Sect. 3.2). For each feature, the mean local contributions are plotted against that feature's value (black line). The importance of each feature (the standard deviation of its contributions) is also included in the form of a gray line. A model visualization provides us with an overview of how features contribute to the model's predictions. For example, observe Fig. 4(a)--the model visualization of the decision tree that was trained on the cDisjunctN data set and is good at predicting the class values. The cDisjunct data set is similar to the cRedundant data set; however, the fourth and fifth Figure 4: Two instance visualizations for the same instance from the cRedundant data set and two different models Figure 3: The naive Bayes model, due to its assumptions of conditional independence of input features, can not model the importance of the equivalence between attr1 and attr2 (both have a zero contribution). Despite this limitation, it correctly predicts the class value, because for this instance, attr5 = 1 is sufficient (this feature has a substantial positive contribution). The artificial neural network correctly models both concepts feature are not copies of the first and second feature. Instead, they are unrelated to the class value. First, the model visualization helps us identify the most important features. The first three features in our example have an equally high importance (gray line--see Sect. 3.2), while the remaining two features are (correctly) identified as of insignificant importance. Second, the plots provide additional information about how features contribute to the model. For example, the first feature (A1) has a negative contribution (speaks against class value 1) if its value is less than 0.5, but contributes positively, if its value is greater than 0.5. Also note that, as shown in Sect. 3.2, if the model is additive, then the plot can also be use to graphically compute the prediction for an arbitrary instance from the data set. The general method simplifies the comparison of different models or types of models. Figure 4(b) depicts a model visualization for an artificial neural network trained on the cDisjunct data set. While the performance of both models (see Fig. 5) is similar with respect to prediction quality, the models are slightly different. The visualization reveals the smooth fit of the artificial neural network and characteristically step-function fit of the decision tree. The artificial neural network also slightly overfit the data as the fourth and fifth feature do slightly influence the models predictions. Figure 5: Model visualizations for two different models and the cDisjunctN data set. Both models learn the concepts behind the data, and the plotted average contribution functions reveal where the individual features’ contribution changes from negative to positive ### User-based experiment The goal of this experiment was to measure if explanations in the form of feature contributions benefit not only (machine learning) experts but also non-expert end-users. Explanations should benefit the user by increasing the user's understanding of the model. The usefulness of explanation methods is usually validated through visual inspection illustrative examples (as we have done in Sect. 4.4) or an application to a real-world problems with domain-expert evaluation. Only a few studies approach the evaluation in a more general and objective way. [12] compared the usefulness of decision tables, binary decision trees, and decision rules in a study that included 51 post-graduate students. The students had to perform understanding and prediction tasks. The authors measured the prediction accuracy, speed of response, and the level of trust, and concluded that decision tables were most useful. Other studies focused only on decision trees [20] and on decision trees and decision rules [2]. Similar to [12], our goal was to measure the effect of the explanation on the user through the users' performance at prediction tasks, which can be measured objectively. We designed the following experiment: * the participant is provided, in sequence, with two sets of instances with predictions and unlabeled instances * for the second set, the participant is also given the situational importance of each feature and all labeled instances, * for each set separately, the participant is asked to learn what the model does from labeled instances (an explanations, if given) and produces predictions for unlabeled instances. We constructed two different sets of instances, T1 and T2 (see Table 3), and used two different variants of the experiment. In the first variant (EXP1), the participant is provided with either T1 or T2 (chosen at random). That is, the participant first solves the task without explanations and then the same task with explanations. In the second variant (EXP2), the participant is provided either with T1 first and T2 second or T2 first and T1 second (chosen at random). That is, the participant solves one task without explanations and then a different task with explanations. A total of 122 computer science students participated in this experiment. The students could only use a pencil and paper to compute their predictions and were limited to 8 minutes per set. All 56 participants in EXP1 were first-year students, which are assumed to have no substantial experience with knowledge discovery. Two groups of students participated in EXP2: 52 1st year students (group A) and 14 4th year students with experience in data mining and knowledge discovery (group B). For each test instance separately, we ranked the mean-squared errors of the participants predictions. We prefer ranks to actual mean-squared errors to avoid the effect of outliers and facilitate comparison across all four test instances. We tested the statistical significance of the differences with the Wilcoxon test (paired for EXP1 and unpaired for EXP2). We use the 95 % confidence level when determining the significance of the results. The results are shown in Table 4. Where explanations were provided, prediction errors rank significantly lower across all groups and both variants of the experiment. Significantly better predictions are a consequence of participants having a better understanding of the model. Given the design of the experiment, it is reasonable to conclude that better understanding was caused by providing explanations. This results suggest that situational importance is a useful form of explanation for non-expert users. It could be argued that in EXP1, better understanding was not a consequence of providing explanations but of participants performing the same task for the second time when explanations were provided. However, in EXP2, this was controlled for by giving participants a different task when explanations were provided. The only potential threat to the validity of this experiment is the possibility that participants matured . However, given the simplicity of the tasks and the short timespan, we assume that this is unlikely. \begin{table} \begin{tabular}{l c c c c} \hline & Without & With & \(p\) value & N \\ \hline EXP2, -, T1 & 36.00 & 20.00 & \(2.2\times 10^{-16}\) & 28 \\ EXP2, -, T2 & 29.50 & 26.5 & \(2.3\times 10^{-4}\) & 28 \\ EXP2, A, T1 & 30.75 & 21.25 & \(6.4\times 10^{-6}\) & 26 \\ EXP2, A, T2 & 28.25 & 23.75 & \(1.5\times 10^{-2}\) & 26 \\ EXP2, B, T1 & 7.75 & 6.62 & \(4.9\times 10^{-2}\) & 7 \\ EXP2, B, T2 & 8.30 & 5.70 & \(1.2\times 10^{-2}\) & 7 \\ \hline \end{tabular} \(p\) values and group sizes (N) are also provided. Prediction errors without explanations (without) rank higher than when explanations are available (with) \end{table} Table 4: Average ranks for the users’ prediction errors for both variants of the experiment and all groups \begin{table} \begin{tabular}{l c c c c c c c c} \hline Instance & T1 & & & & T2 & & & \\ \cline{2-9} & A1 & A2 & A3 & C & A1 & A2 & A3 & C \\ \hline learn1 & 11.76 & 70 & 12 & 52 & A & 450 & 21 & 27 \\ \(\varphi\) & 0 & 0 & \(-60\) & – & \(-31\) & +11 & 0 & – \\ learn2 & 11.56 & 55 & 16 & 82 & A & 250 & 21 & 2 \\ \(\varphi\) & 0 & 0 & \(-30\) & ## 5 Conclusion We proposed a general method for explaining how features contribute to classification and regression models' predictions. The method builds on previous work on a general method for computing the situational importance of features for prediction models. By design, the method perturbs all subsets of features to deal with the shortcomings of other existing general methods that do not properly take into account interactions between features. We derived the mean situational importance of a feature's value, and we show how it can be used as a basis for a model visualization, which provides an overview of how features contribute to the model's predictions. For additive models, this approach generalizes previous additive model-specific methods and general explanation methods. We also proposed two enhancements to the sampling algorithm (quasi-random and adaptive sampling) that reduce the running time of the algorithm. Empirical results across several types of models and data sets show that the method is an efficient and useful tool for visualizing models, comparing different types of models, and identifying potential errors. Furthermore, an experiment with human participants showed that providing an explanation in the form of feature contributions increased the user's understanding of the prediction model. ## References * (1) Achen CH (1982) Interpreting and Using Regression. Sage Publications, Thousand Oaks * (2) Allahyari H, Lavesson N (2011) User-oriented assessment of classification model understandability. In: Proceedings of the 11th Scandinavian conference on artificial intelligence, SCAI 2011, pp 11-19 * (3) Becker B, Kohavi R, Sommerfield D (1997) Visualizing the simple Bayesian classier. KDD workshop on issues in the integration of data mining and data visualization * (4) Bhattacharya S, Xu D, Kumar K (2011) An ANN-based auditor decision support system using Benford's law. Decis Support Syst 50(3):576-584 * (5) Bhattacharyya S, Jha S, Tharakunnel K, Westland JC (2011) Data mining for credit card fraud: a comparative study. Decis Support Syst 50(3):602-613 * (6) Blanchard J, Guillet F, Briand H (2007) Interactive visual exploration of association rules with rule-focusing methodology. Knowl Inf Syst 13:43-75 * (7) Castro J, Gomez D, Tejada J (2009) Polynomial calculation of the shapley value based on sampling. Comput Oper Res 36(5):1726-1730 * (8) De Falco I, Della Cioppa A (2005) An evolutionary approach for automatically extracting intelligible classification rules. Knowl Inf Syst 7:179-201 * (9) Frank A, Asuncion A (2011) Uci machine learning repository * (10) Hall M, Frank E, Holmes G, Pfahringer B, Reutemann P, Witten IH (2009) The weka data mining software: an update. SIGKDD Explor Newst 11(1):10-18 * (11) Huang Z, Chen H, Hsu Cl, Chen WH, Wu S (2004) Credit rating analysis with support vector machines and neural networks: a market comparative study. Decis Support Syst 37(4):543-558 * (12) Huysmans J, Dejaeger K, Mues C, Vanthienen J, Baesens B (2011) An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models. Decis Support Syst 51(1):141-154 * (13) Jaeckel P (2002) Monte Carlo methods in finance. Wiley, New York * (14) Jakulin A, Mozina M, Demsar J, Bratko I, Zupan B (2005) Nomograms for visualizing support vector machines. KDD '05: 11th ACM SIGKDD, ACM, pp 108-117 * (15) Kattan MW, Eastham JA, Stapleton AM, Wheeler TM, Scardino PT (1998) A preoperative nomogram for disease recurrence following radical prostatectomy for prostate cancer. J Natl Cancer Inst 90:766-771 * (16) Knuth DE (1998) The art of computer programming, volume 2: seminumerical algorithms. Addison-Wesley, Boston * (17) Kononenko I (1993) Inductive and bayesian learning in medical diagnosis. Appl Artif Intell 7:317-337 * (18) Lee S (2010) Using data envelopment analysis and decision trees for efficiency analysis and recommendation of B2C controls. Decis Support Syst 49(4):486-497 * (19) Lemaire V, Feraud R, Voisine N (2008) Contact personalization using a score understanding method. In: International joint conference on neural networks (IJCNN) * (20) Lim BY, Dey AK, Avrahami D (2009) Why and why not explanations improve the intelligibility of context-aware intelligent systems. In: Proceedings of the 27th international conference on Human factors in computing systems, CHI '09, ACM, New York, NY, USA, pp 2119-2128 * (21) Lubsen J, Pool J, van der Does E (1978) A practical device for the application of a diagnostic or prognostic function. Methods Infl Med 17:127-129 * (22) Melli G (n.d.) The datgen dataset generator. http://www.datasetgenerator.com * (23) Mozina M, Demsar J, Katatan M, Zupan B (2004) Nomograms for visualization of naive Bayesian classifier. PKDD 2004, Springer, pp 337-348 * (24) Robnik-Sikonja M, Kononenko I (2008) Explaining classifications for individual instances. IEEE TKDE 20:589-600 * (25) Shapley LS (1953) A value for n-person games, vol II of Contributions to the theory of games. Princeton University Press, Princeton * (26) Szafron D, Poulin B, Eisner R, Lu P, Greiner R, Wishart D, Fyshe A, Pearcy B, Macdonell C, Anvik J (2006) Visual explanation of evidence in additive classifiers. In: Proceedings of innovative applications of artificial intelligence * (27) Strumbelj E, Bosnic Z, Zakotnik B, Grasic-Kuhar C, Kononenko I (2010) Explanation and reliability of breast cancer recurrence predictions. Knowl Inf Syst 24(2):305-324 * (28) Strumbelj E, Kononenko I (2010) An efficient explanation of individual classifications using game theory. J Mach Learn Res 11:1-18 * (29) Strumbelj E, Kononenko I (2011) A general method for visualizing and explaining black-box regression models. In: Dobnikar A, Lotric U, Ster B (eds) ICANNGA (2), vol 6594 of Lecture notes in computer science. Springer, Berlin, pp 21-30 * (30) Welford BP (1962) Note on a method for calculating corrected sums of squares and products. Technometrics 4(3):419-420 \begin{tabular}{c c} & Erik Strumbelj received his Ph.D. in computer science from University of Ljubljana, Slovenia, in 2011. He is currently an assistant professor at the Faculty of Computer and Information Science, University of Ljubljana. His research interests include machine learning and applied statistics. \\ \end{tabular} Igor Kononenko received his Ph.D. in 1990 in computer science from University of Ljubljana, Slovenia. He is a professor at the Faculty of Computer and Information Science in Ljubljana. His research interests include artificial intelligence, machine learning, neural networks, and cognitive modeling. He is the (co)author of 200 journal and conference papers and 12 textbooks in these fields."
1,"# Analysis of regression in game theory approach Stan Lipovetsky12 and Michael Conklin Stan Lipovetsky312 and Michael Conklin312 Stan Lipovetsky412 and Michael Conklin412 Many different techniques are used in applied regression analysis to evaluate the relative importance of the predictors. One particularly useful technique is a decomposition of the coefficient of multiple determination into direct, indirect and net effects associated with each variable [2]. The net effect of a predictor is a combination of the direct (as measured by its regression coefficient squared) and the indirect effects (measured by the combination of its correlations with other variables). The net effects have the nice property of summing to the total coefficient of multiple determination \(R^{2}\) of the model. They explicitly take into account the correlations that predictor variables have with each other. However, the net effect values themselves are influenced by the collinear redundancy in the data so that the estimated net effects can be negative, that is difficult to interpret. On the other hand, even in presence of multicollinearity, it is often desirable to keep available variables in the model and to estimate comparative importance of their relation to the dependent variable. It makes sense because all variables do not represent each other exactly, rather each of them plays its own specific role in fitting and understanding behaviour of the dependent variable. Another important issue is that in most real-world situations, researchers and practitioners have neither a comprehensive theory, nor a complete control over all variables that could describe numerous specific features of a complex object or process (see, for example, References [3,4]). When a model does not incorporate some variables that correlate with included variables, regression estimates are biased and inconsistent (see References [5, pp. 334-350; 6, pp. 24-25]). However, in the absence of knowing what variables are necessary for the equation, we can consider different models, even limiting consideration to linear regressions. Also some kind of averaging of the estimated characteristics over all the possible models can be applied. Various techniques were elaborated for choosing and averaging among the regression models to find the best subset [7, 8, 9, 10]. However, given the power reduction caused by multicollinearity it is difficult to be sure that the best models found by such search are really superior to the many other models that are rejected. Many techniques have also been proposed for combining alternative models. Bagging, or bootstrap aggregation [11], boosting [12] and bundling [13] are just some of the techniques that have been shown to improve predictions over choosing a single model. The disadvantage in these techniques is that inference about specific variables is difficult. In bootstrapping experiments we can demonstrate that a model with typical levels of multicollinearity could consist of spurious coefficients corresponding to a random distribution around zero. In the presence of multicollinearity, the standard error of every coefficient can be several times more and at least not less than the coefficient itself. This means that an actually arbitrary decision could be made based on the analysis of the regression coefficients or their shares of influence. Thus, we need a decision tool that can produce clear results for estimation of regressors even if they are collinear, and when there actually are many possible models by various subsets of the predictors. The appropriate tool we find in the co-operative game theory. We can think of the particular model as a way of building coalitions among players (predictor variables) to maximize the total value (quality of fitting). In the field of co-operative games a useful analysis and decision tool is the Shapley Value imputation [14, 15, 16, 17]. Results of the regression calculations can serve as the initial data to finding Shapley Value that in its turn reshapes the regression net effects and coefficients. We proved this technique to be useful during several years of solving various complicated problems in the marketing research field [18, 19, 20, 21, 22, 23]. Of course, some other game-theoretical techniques can be used in statistical decisions under consideration, but we prefer the Shapley Value imputation because it is not an heuristic procedure, it was derived as an axiomatic approach, and it produces a unique solution satisfying general requirements of Nash equilibrium [14,15]. This paper is organized as following. In Section 2 we describe how to estimate the predictors importance, and in Section 3 we consider Shapley Value analysis. Its application to net effects and a possibility to adjust regression coefficients is considered in Section 4. Numerical example of Shapley Value in regression analysis with bootstrapping estimation is given in Section 5. In Section 6 we summarize results of Shapley Value application to analysing regression models. ## 2 Predictors Contribution in Regression Let us consider briefly some properties of linear regression that will be used in further analysis. A multiple regression model is \[y_{i}=\beta_{1}x_{1i}+\beta_{2}x_{2i}+\ \cdots\ +\ \beta_{n}x_{ni}+e_{i}\] (1) where \(y\) and all \(x\)'s are standardized variables (centred and normalized by standard deviations), \(e\) denotes normal random error and \(i\) is a number of observation (\(i=1,2,\ldots,N\)). Coefficients of the regression can be found in the least-squares (LS) approach that corresponds to minimization of the objective \[S^{2}=e^{\prime}e=(y-Xb)^{\prime}(y-Xb)=1-2b^{\prime}r+b^{\prime}Cb\] (2) where \(y\) and \(e\) are the \(N\)th-order vector-columns of the dependent variable and error term, respectively, \(X\) is the matrix of \(N\times n\) order of standardized independent variables, \(b\) is the \(n\)th-order vector-column of LS estimates for beta-coefficients of the regression (1), \(C=X^{\prime}X\) is the \(n\)th-order matrix of sample correlations \(r_{jk}\) among all pairs of variables \(x_{j}\) and \(x_{k}\), vector-column \(r=X^{\prime}y\) of size \(n\) defines sample correlations \(r_{j}\) between each \(x_{j}\) and the dependent variable \(y\) and prime denotes transposition. Minimizing (2) yields the normal system of equations \[Cb=r\] (3) and solution of system (3) is \[b=C^{-1}r\] (4) where \(C^{-1}\) is the inverted matrix of correlations. Minimizing deviations (2) is equivalent to maximizing of the regression quality estimated by the coefficient of multiple determination \(R^{2}\) (this problem in the context of canonical analysis is discussed in References [24,25,26]), that could be presented as \[R^{2}=1-S^{2}=2b^{\prime}r-b^{\prime}Cb=\sum_{j}\ b_{j}\,(2r-Cb)_{j}\] (5) and reduced to the same solution (3) and (4). Substituting (3) into (5), we find the maximum of the multiple determination as the scalar product of vectors \(b\) and \(r\): \[R^{2}=b^{\prime}r=\sum_{j}\,b_{j}r_{j}\] (6) Items of the total \(R^{2}\) (6) define the so-called net effects (NEF) of each \(j\)th regressor \[{\rm NEF}_{j}=b_{j}r_{j}\] (7) Multicollinearity can change a sign of \(b_{j}\) in the multiple regression to opposite in comparison with the pairwise regression \(y\) by \(x_{j}\) (pair regression coefficient coincides with the correlation \(r_{j}\)), then \(j\)th net effect becomes negative. The values of multiple determination (6) and net effects (7) are widely used in practice of regression modelling, although there are theoretically more reasonable indices for evaluation of predictor importance. For example, the share of \(x_{j}\) could be defined by the square of the partial correlation between \(y\) and \(x_{j}\) with fixed other \(x\)'s (see References [27, 28]): \[R^{2}_{{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{\it{ \it{\it{ \it{ \it{ }}}}}}}}}}}}}}=(R^{2}-R^{2}_{-j})/(1-R^{2}_{-j})\] (8) where \(R^{2}\) denotes multiple determination in model (1) with all \(n\) predictors including \(x_{j}\), and \(R^{2}_{-j}\) denotes multiple determination in the model where in denominator we have the \(j\)th diagonal element of the inverted correlation matrix \(C^{-1}\) (4) of all the \(x\)'s. In the presence of highly correlated variables among the \(x\)'s, the matrix \(C\) (3) degenerates to the ill-conditioned matrix, and solution (4) produces poor results both for coefficients of regression and for net effects. In the limit of very high multicollinearity among the \(x\)'s solution (4) simply does not exist, and the techniques of the ridge regression analysis can be recommended (for example, References [9,10,32]). Let us consider now new possibilities that the Shapley Value analysis suggests for estimation of regressors' shares in their mutual influence on the dependent variable, and for evaluation of the coefficients in the regression model. ## 3. Shapley Value Imputation The Shapley Value, hereafter referred to as SV, was developed to evaluate an ordering of the worth of players in a multiplayer co-operative game. The key to understanding its utility is that it represents the worth of each player over all possible combinations of players. Extending this to the problem of comparative usefulness of regressors, the SV assigns a value for each predictor calculated over all possible combinations of predictors in regressions. The SV approach to the problem provides a solution that is closer to the actual modeling for any complex process or object, because it compares and averages over all possible subsets of predictors in the model. This is an advantage of the SV solution because by comparing across all possible models it includes the possibility of competitive influence of any subsets of predictors in the analysis. The Shapley Value is defined as each \(j\)th participant's input to a coalition \[S_{j}=\sum_{\text{all }M}\gamma_{n}(M)\left[\upsilon\left(M\cup\{j\}\right)- \upsilon\left(M\right)\right]\] (13) with weights of proportions to enter into a coalition \(M\) defined as \[\gamma_{n}(M)=m!(n-m-1)!/n!\] (14) In (13) and (14), \(n\) is the total number of all the participants, \(m\) is the number of participants in the \(M\)th coalition, and \(\upsilon\) ( ) is the characteristic function used for estimation of utility for each coalition. By \(M\cup\{j\}\) a set of participants which includes the \(j\)th participant is denoted, when \(M\) means a coalition without the \(j\)th participant. In our case, the participants of the coalition game are predictors incorporated into the regression model. Regression output supplies us with \(R^{2}\) values, or per cent of explained variability reached for each set of variables in regression modelling. For ease of exposition, let us use notations \(A\), \(B\) and \(C\), etc. for variables \(x_{1}\), \(x_{2}\) and \(x_{3}\), etc., respectively. Then \(R^{2}_{ABC}\), for example, defines the multiple determination in model (1) with the regressors \(A\), \(B\) and \(C\) (or, the same, \(x_{1}\), \(x_{2}\) and \(x_{3}\)). We define characteristic function \(\upsilon\) (13) via these \(R^{2}\) values estimated by the results of regression modelling. Let us construct, for the example of \(n=5\), the characteristic function for the variable \(A\) (where there are other variables \(B\), \(C\), \(D\) and \(E\)): \[\upsilon\left(0\right)=0,\quad\upsilon\left(A\right)=R^{2}_{A},\quad\upsilon \left(AB\ where all the right-hand-side values are estimations of the multiple determination coefficients in different regressions containing the regressor \(A\). Substituting characteristic function (15) into SV expression (13), we can see that each item in brackets (13) actually coincides with the usefulness defined in (9). That means that SV for a predictor \(A\) is a measure of its usefulness averaged by all the models that contain this regressor \(A\). Weights of imputation (14) for \(n=5\) are \[\gamma(0)=\gamma(4)=0.20,\ \ \ \gamma(1)=\gamma(3)=0.05,\ \ \ \gamma(2)=0.033\] (16) Then the SV (13) for the variable \(A\) can be written explicitly as \[S_{A}=0.2(U_{A})+0.05(U_{AB}+U_{AC}+U_{AD}+U_{AE})\] \[+0.033(U_{ABC}+U_{ABD}+U_{ABE}+U_{ACD}+U_{ACE}+U_{ADE})\] \[+0.05(U_{ABCD}+U_{ABCE}+U_{ACDE}+U_{ABDE})+0.2(U_{ABCDE})\] (17) where the values of usefulness (9) for our sets of regressors are as follows: \[U_{A}=R_{A}^{2},\ \ \ U_{AB}=R_{AB}^{2}-R_{B}^{2},\ldots,\ U_{ABC}=R_{ABC}^{2}-R_ {BC}^{2},\ldots\] \[U_{ABCD}=R_{ABCD}^{2}-R_{BCD}^{2},\ldots,\ U_{ABCDE}=R_{ABCDE}^{2}-R_{BCDE}^ {2}\] (18) The items in sum (17) correspond to usefulness' margins from the variable \(A\) to all the coalitions, and the Shapley Value imputation \(S_{A}\) corresponds to the mean margin of the variable \(A\), estimated by averaging over its possible participation in all coalitions. Similar formulas are used for each of the other variables \(B\), \(C\), \(D\) and \(E\), and their Shapley Values (13) define margins from each of these regressors. The total of the margins from all the variables equals the maximum value of \(R^{2}\) in the model with all the regressors together, that is (due to (15)) \[\sum_{k}^{n}S_{k}=\upsilon\,(\mbox{all})=R_{ABCDE}^{2}\] (19) Thus, the SV are shares of the total \(R^{2}\) and they define importance of each regressor in the model. Regrouping the items in (17) with help of (18), we represent the Shapley Value imputation formula in the following form: \[S_{A}=(R_{A}^{2}-\bar{R}_{1}^{2})/(n-1)+(\bar{R}_{A^{*}}^{2}-\bar{R}_{2}^{2})/ (n-2)+(\bar{R}_{A^{**}}^{2}-\bar{R}_{3}^{2})/(n-3)\] \[+\ \cdots\ +(\bar{R}_{A^{**}}^{2}-\bar{R}_{n^{-}1}^{2})/(n-(n-1))+R_{AB\, \ldots\,N}^{2}/n\] (20) In the first item of sum (20) we see a difference of \(R_{A}^{2}\) for the model with one regressor A and mean value of \(\bar{R}_{1}^{2}\) (marked by bar over \(R^{2}\)) for all the models with just one regressor (marked by sub-index 1). In the second item of sum (20) we see is a difference between mean \(\bar{R}_{A^{*}}^{2}\) for all the models with two regressors one of which is \(A\) (marked by sub-index \(A\)* with asterisk denoting any other variable \(x\)), and mean \(\bar{R}_{2}^{2}\) for all the models with any two regressors (marked by sub-index 2), etc. The last item represents a share that the regressor A has in the total \(R^{2}\) of model (1) with all the \(x\)'s together. ## 4 SV NET EFFECTS and Coefficients of Adjusted Regression Suppose, we found SV (20) for each regressor, with their total equals multiple determination (19) for model (1) with all the variables. These Shapley Values (19) are nothing else but estimations of the net effects (7) obtained via averaging by all possible models in the co-operative game approach. Returning from lettered indices to the index \(j\), let us denote by SV\({}_{j}\) the net effects estimated by Shapley Value imputation for each regressor. Then in place of (6) and (7) for regular net effects for multiple regression (1) we can write decomposition of the multiple determination by the net effects estimated as Shapley Values \[R^{2}=\sum_{j}\,SV_{j}\] (21) Each item in (21) is a very stable estimate of net effect because Shapley Value is an average across all possible linear models with different subsets of the regressors. Thus, it is not so volatile as regular net effect and is not prone to multicollinearity distortion. In comparison to net effects (7), SV net effects (21) are always positive, so they are interpretable and suggest an easy way of graphical (pie-charts) presentation of regressors' shares in their contribution to explanation of the behaviour of dependent variable \[\sum_{j}\,(\mbox{SV}_{j}/R^{2})=1\] (22) Let us consider a simple possibility of estimating which of the shares (22) are statistically significant, or which of the regressors are important in their contribution to \(R^{2}\). Suppose we take level of significance \(\alpha\) (for example, 5 per cent) for checking the difference of the coefficient \(R^{2}\) (21) from zero. In the assumption of independence of the items in multiple comparison and for equal confidential probability \(1-\gamma\) for each of them, we can obtain the joint probability \(1-\alpha\) as the product of the probabilities of all the items \((1-\gamma)^{n}\). Then the so-called Bonferroni confidential interval [33] for each item among \(n\) of them can be defined as \[\gamma=1-(1-\alpha)^{1/n}\] (23) that for \(n>1\) is always less than \(\alpha\) level. Standard deviation \(\sigma_{R}\) for the coefficient of multiple regression \(R\) (square root of multiple determination \(R^{2}\)) equals \[\sigma_{R}=\sqrt{(1-R^{2})/(N-n-1)}\] (24a) where \(N-n-1\) is the number of degrees of freedom. Let us denote sample \(t\)-value for \(R\) as \(t_{R}=R/\sigma_{R}\). On the level of significance \(\gamma\) (23) the interval estimates for \(R\) are \[R_{\pm}=R\pm t_{\gamma/2}\sigma_{R}\] (24b) with two-tailed \(t\)-statistics \(t_{\gamma/2}\). Then the relative squared deviation is \[\delta^{2}=((R_{\pm}-R)/R)^{2}=t_{\gamma/2}^{2}\sigma_{R}^{2}/R^{2}=t_{\gamma/ 2}^{2}/t_{R}^{2}\] (25) that equals the ratio of critical and empirical squared \(t\)-values. The shares of net effects (22) higher than the threshold (25), \(\mathrm{SV}_{j}/R^{2}>\delta^{2}\), can be considered as important (significantly different from zero), and those \(\mathrm{SV}_{j}/R^{2}<\delta^{2}\) correspond to the variables that can be neglected. Let us consider adjusting regression coefficients by SV net effects. Using calculated \(\mathrm{SV}_{j}\), we can rewrite relation (7) for net effect as \(\mathrm{SV}_{j}=a_{j}r_{j}\) where by \(a_{j}\) we denote unknown parameters of an adjusted regression with such a property: product of each coefficient \(a_{j}\) with correlations \(r_{j}\) yields Shapley Value net effect. Then solving these simple equations we have: \[a_{j}=\mathrm{SV}_{j}/r_{j}.\] (26) Coefficients (26) are obtained via definition (7) for net effects. However, this definition (7) corresponds to items in expression (6) for multiple determination obtained as a result of substitution of the least-squares normal system of Equations (3) into the general objective for \(R^{2}\) (5). To re-estimate coefficients of the regression with the obtained Shapley Values, we suggest to use not a simple relation \(\mathrm{SV}_{j}=a_{j}r_{j}\) but a more complicated expression for net effect defined as the items in sum (5) with the coefficients \(a_{j}\). So we can write equations for finding coefficients of regression adjusted by Shapley Values as follows: \[a_{j}\left(2r-Ca\right)_{j}=\mathrm{SV}_{j}\] (27) For already found Shapley Values, relations (27) present a system of \(n\) quadratic equations. This system can be solved for \(a_{j}\) by a non-linear minimizing of the objective \[F=\sum_{j}\,[\mathrm{SV}_{j}-a_{j}(2r-Ca)_{j}]^{2}=\sum_{j}\,(\mathrm{SV}_{j} -2a_{j}r_{j}+a_{j}\sum_{k}\,r_{jk}a_{k})^{2}\] (28) Coefficients (26) can be used as the initial approximation \(a_{j}^{(0)}\) in the minimizing procedure (28). Parameters \(a_{j}\) obtained in (28) are coefficients of the adjusted regression estimated via Shapley Values. These coefficients are not prone to distortion from multicollinearity, and have interpretable signs and values. Let us consider a convenient characteristic of the difference between two solutions for regression coefficients. It can be constructed as a ratio of the residual sum of squares for a non-least-squares regression to the residual sum of squares of the least-squares regression (see References [31,34]). Denote residual sum of squares in model (1) as \(S^{2}(a)\) for the solution \(a\) obtained by (28), and \(S^{2}(b)\) for beta-coefficients \(b\) (4). Using norms of vectors, we have \[S^{2}(a) =\|y-Xa\|^{2}=\|(y-Xb)-X(a-b)\|^{2}=\|y-Xb\|^{2}-2(a-b)^{\prime}X^ {\prime}(y-Xb)\] \[\qquad+(a-b)^{\prime}X^{\prime}X(a-b)=S^{2}(b)+(a-b)^{\prime}C(a-b)\] (29) In derivation (29) we use a property \(X^{\prime}(y-Xb)=0\) satisfied due to solution (3). From (29) we get a relative index \[S^{2}(a)/S^{2}(b)=1+((a-b)^{\prime}\,C(a-b))/(1-R^{2})\] (30) This expression defines efficiency of an adjusted regression versus least-squares model: if value (30) equals \((1+d)\) per cent, than adjusted regression has \(d\) per cent bigger residual variance than regular regression. This is the price of the trade-off for an adjusted regression with interpretable coefficients and positive net effects. ## 5 Numerical Estimations Formula (20) presents Shapley Value as a marginal input from each variable averaged by all possible coalitions. The important feature of this formula is the presentation of subsequent inputs of coalitions of the first, second, etc., levels to the total Shapley Value. If the data is available only on the several initial stages of coalitions with one, two, and some other subsets of variables, it is possible to use (20) for estimation of partial inputs to the total SV. Comparison of such cumulative values for each variable \(A\), \(B\), \(C\), etc., allows to evaluate stability of the SV from partial data. This suggests an approach for reducing the computation time of the SV by limiting computation to the number of levels where stability is achieved. We can see by (20) that each term is constructed by calculating a mean value of combinations with the product and a mean value of combinations without it, thus, we can estimate those means by sampling combinations. This could be easily done and incorporated in the code whenever the number of regressors being evaluated is above 10 (see Reference [19]). Let us consider an example of a real project on customer satisfaction study for a telephone customer service center. The variables are measured in scale from 1 (totally dissatisfied, or disagered) to 7 (totally satisfied, or agreed). They include: \(y\)--overall satisfaction of clients with the company in general (dependent variable); \(x1\)--customer satisfaction with service representatives; \(x2\)--service representatives are courteous; \(x3\)--they provide all the needed information; \(x4\)--they give quick response; \(x5\)--they show care with customer problems; \(x6\)--they are accurate in the answers; \(x7\)--they take all the necessary actions. The data gathered by 242 respondents is available. All variables are positively correlated (pair correlations are from 0.52 to 0.89). The aim of the modeling was to measure the input of the predictors in their influence on the dependent variable. In Table I some numerical results are presented. In the first row of this table we see that dependent variable is correlated with all the regressors rather evenly, so each variable can be more or less equally important in the model. However, by beta-coefficients and their \(t\)-statistics in the next two rows it is clear that most of the predictors are insignificant in the model. Two \(x\)'s have negative beta-coefficients, and their net effects are negative too (next two rows in Table I), although pair correlations (and pairwise regressions of \(y\) by each \(x\) separately) have positive signs and similar values. Of course, it is the effects of multicollinearity, but knowing it does not help much in comparison of the variables' importance. And what do we do if we want to construct a pie-chart by shares of net effects with two of those negative? Multiple determination equals 0.356 and its \(t\)-statistics is \(t_{R}=11.52\) (see Table I), so the model is good for forecasting, but it is hardly useful for the analysis of the regressors. The next rows of Table I show the Shapley Value net effects--they are positive, thus, interpretable. Moreover, the net effects become closer to one another--it makes more sense taking into account similar values of the correlations with the dependent variable (Table I, the first row). These net effects can be used without any difficulties, particularly in graphical presentation of the regressors contribution into the model. Taking confidential probability \(\alpha=5\) per cent, we find by (23) confidential probability for net effects \(\gamma=0.73\) per cent, so \(t_{\gamma/2}=2.68\). The relative index \(\delta^{2}\) (25) equals 5.84 per cent, so all Shapley Value net effect shares are above this level (see Table I), thus, the contribution from each regressor is significant in the model. Using Shapley Value, we also construct the adjusted coefficients of regression in the approach (26) and (28). We see (in the last two rows of Table I) that they are very close (although \(a_{j}\) (28) was obtained after several dozens of iterations by the 'nlminb' function in SPLUS software). Using the final set of coefficients we estimate that multiple determination for this model equals 0.345 (last row and last column in Table I)--it is just a little less than the original value of 0.356. Efficiency index (30) equals 1.017, i.e. residual variance of the adjusted regression is just 1.7 per cent higher than that of a regular regression, but at the same time all re-estimated parameters of regression become positive and interpretable. Additionally, we use bootstrapping for evaluation of the considered characteristics--see Table II. Again Shapley Value, based on the averaged sets of regressors, demonstrates stable results and robust decision rule. In Table II we represent beta-coefficients, net effects, Shapley Value net effects, and adjusted coefficients of regression (the first row in each group). By bootstrapping with 50 replications we estimated mean values and standard deviation of all characteristics--see the second and the third rows in each group of the results. The \(t\)-ratio of means to standard deviations is shown in the last row of each group. By the results in Table II we see a high volatility of beta-coefficients and regressor net effects--these characteristics are biased from their means, and the means are usually less than their standard deviations. At the same time all characteristics obtained via Shapley Value (SV net effects and adjusted coefficients of regression) are very stable. They are almost unbiased from their mean values, and these means are several times more than the corresponding standard errors. Therefore, when using SV net effects and adjusted coefficients of regression we can be sure of the contribution of individual regressors and in the estimation of the regressors' influence on the dependent variable. \begin{table} \begin{tabular}{l c c c c c c c c} \hline Predictor & \(X1\) & \(X2\) & \(X3\) & \(X4\) & \(X5\) & \(X6\) & \(X7\) & \(R^{2}\) \\ \hline Correlation \(r_{j}\) & 0.543 & 0.450 & 0.545 & 0.433 & 0.511 & 0.546 & 0.505 & — \\ Beta (4) & 0.255 & \(-\) 0.022 & 0.177 & \(-\) 0.039 & 0.05 ## 6 Summary We considered application of a tool from the cooperative game theory, namely, Shapley Value analysis, for evaluation of coefficients of regression and relative usefulness of the predictors in the model. The results are very encouraging--they show that it is possible to perform a reliable analysis even with a high degree of multicollinearity. The results can be understood by the specific structure of Shapley Value inputs as averages of the net effects over all possible coalitions of regressors. While regular regression coefficients and shares of importance are highly prone to multicollinearity distortions, all Shapley Value characteristics, being averaged values, are very consistent and demonstrate very stable bootstrapping output. Due to several years of our experience actively using the described approach, the Shapley Value technique can be successfully combined with multiple regression, significantly facilitating analysis of the regression models in numerous practical applications. ## Acknowledgements The authors wish to thank a referee for the suggestions that improved the paper. ## References * [1] Mason ChH, Perreault Jr WD. Collinearity, power, and interpretation of multiple regression analysis. _Journal of Marketing Research_ 1991; **28**:268-280. * [2] Ferber R. _Marketing Research_. Ronald Press: New York, 1964. * [3] Tishler A, Lipovetsky S. The flexible CES-GBC family of cost functions: derivation and application. _The Review of Economics and Statistics_ 1997; **LXXIX**:638-646. \begin{table} \begin{tabular}{l c c c c c c c} Predictor & \(X1\) & \(X2\) & \(X3\) & \(X4\) & \(X5\) & \(X6\) & \(X7\) \\ Beta (4) & \(0.255\) & \(-\) \(0.022\) & \(0.177\) & \(-\) \(0.039\) & \(0.052\) & \(0.195\) & \(0.029\) \\ Mean & \(0.183\) & \(0.013\) & \(0.203\) & \(-\) \(0.035\) & \(0.090\) & \(0.172\) & \(0.045\) \\ Std & \(0.246\) & \(0.170\) & \(0.210\) & \(0.127\) & \(0.178\) & \(0.187\) & \(0.154\) \\ \(t\) & \(0.74\) & \(0.08\) & \(0.97\) & \(-\) \(0.27\) & \(0.51\) & \(0.92\) & \(0.29\) \\ NEF (7) & \(0.139\) & \(-\) \(0.010\) & \(0.096\) & \(-\) \(0.016\) & \(0.027\) & \(0.106\) & \(0.014\) \\ Mean & \(0.145\) & \(0.018\) & \(0.106\) & \(-\) \(0.002\) & \(0.039\) & \(0.097\) & \(0.013\) \\ Std & \(0.135\) & \(0.071\) & \(0 * [4] Tishler A, Lipovetsky S. A globally concave, monotone and flexible cost function: derivation and application. _Applied Stochastic Models in Business and Industry_ 2000; **16**:279-296. * [5] Kmenta J. _Elements of Econometrics_. Macmillan: New York, 1986. * [6] Long JS. _Regression Models for Categorical and Limited Dependent Variables_. SAGE Publications: London, 1997. * [7] Madigan D, Raftery AE. Model selection and accounting for model uncertainty in graphical models using Occam's window. _Journal of American Statistical Association_ 1994; **89**:1535-1546. * [8] Raftery AE, Madigan D, Hoeting JA. Bayesian model averaging for linear regression model. _Journal of American Statistical Association_ 1997; **92**:179-191. * [9] Lipovetsky S, Conklin M. CRI: a collinearity resistant implement for analysis of regression problems. _Proceedings of the 31st Symposium on the Interface: Computing Science and Statistics_, Schaumburg, IL, 9-12 June, 1999; 282-287. * [10] Lipovetsky S, Conklin M. Multiobjective regression modifications for collinearity. _Computers and Operations Research_, 2001, forthcoming. * [11] Breiman L. Bagging predictors. _Machine Learning_ 1994; **26**:123-140. * [12] Freund Y, Schapire RE. A decision-theoretic generalization of on-line learning and an application to boosting. _Journal of Computer and System Sciences_ 1995; **55**:119-139. * [13] Lee SS, Elder JF. Bundling heterogeneous classifiers with advisor perceptrons. Elder Research Technical Report, University of Idaho, 1997. * [14] Myerson RB. _Game Theory_. _Analysis of Conflict_. Harvard University Press: Cambridge, MA and London, England, 1991. * [15] Owen G. _Game Theory_. Academic Press: New York, 1982. * [16] Roth AE. The Shapley Value as a Von Neumann-Morganstern utility. _Econometrica_ 1977; **45**:657-664. * [17] Roth AE. (ed.). _The Shapley Value: Essays in Honor of Lloyd S. Shapley_. Cambridge University Press: Cambridge, 1988. * [18] Conklin M, Lipovetsky S. Choosing product line variants: a game theory approach. _Proceedings of the 30th Symposium on the Interface: Computing Sciences and Statistics: Dimension Reduction, Computational Complexity and Information_. Minneapolis, MN, Vol. 30. 1998; 164-168. * [19] Conklin M, Lipovetsky S. Modern marketing research combinatorial computations: Shapley Value versus TURF tools. _Proceedings of 1998 International S-Plus User Conference_, MathSoft Inc., Washington, DC, 8-9 October 1998. * [20] Conklin M, Lipovetsky S. A winning tool for CPG"". _Marketing Research: A Magazine of Management and Applications_ 2000; **11**:23-27. * [21] Conklin M, Lipovetsky S. A new approach to choosing flavors. _Proceedings of the 11th Annual Advanced Research Techniques Forum of the American Marketing Association_, Monterey, CA, 4-7 June 2000. * [22] Conklin M, Lipovetsky S. Identification of key dissatisfiers in customer satisfaction research. _The 11th Annual Advanced Research Techniques Forum of the American Marketing Association_, Monterey, CA, 4-7 June 2000. * [23] Conklin M, Lipovetsky S. Evaluating the importance of predictors in the presence of multicollinearity. _Proceedings of the 12th Annual Advanced Research Techniques Forum of the American Marketing Association_, Amelia Island, FL, 24-27 June 2001. * [24] Lipovetsky S, Tishler A. Linear methods in multimode data analysis for decision making. _Computers and Operations Research_ 1994; **21**:169-183. * [25] Tishler A, Lipovetsky S. Canonical correlation analysis for three data sets: a unified framework with application to management. _Computers and Operations Research_ 1996; **23**:667-679. * [26] Tishler A, Lipovetsky S. Modelling and forecasting with robust canonical analysis: method and application. _Computers and Operations Research_ 2000; **27**:218-232. * [27] Goldberger A. _Econometrics_. Wiley: New York, 1964. * [28] Timm N. _Multivariate Analysis with Applications in Education and Psychology_. Brooks-Cole: Monterey, CA, 1975. * [29] Darlington R. Multiple regression in psychological research and practice. _Psychological Bulletin_ 1968; **79**:161-182. * [30] Harris R. _A Primer of Multivariate Statistics_. Academic Press: New York and London, 1975. * [31] Weisberg S. _Applied Linear Regression_. Wiley: New York, 1985. * [32] Marquardt D. Generalized inverses, ridge regression and biased linear estimation. _Technometrics_ 1970; **12**:591-612. * [33] Hsu JC. _Multiple Comparisons: Theory and Methods_. Chapman & Hall: London, 1996. * [34] Ehrenberg ASC. How good is best. _Journal of Royal Statistical Society_, **4**, 1982; **145**:364-366. * [35] Cooley W, Lohnes P. _Multivariate Data Analysis_. Wiley: New York, 1971. * [36] Grapentine T. Managing multicollinearity. _Marketing Research_ 1997; 11-21. * [37] Green PE, Carroll JD, DeSarbo WS. A new measure of predictor variable importance in multiple regression. _Journal of Marketing Research_ 1978; **20**:356-360."
